{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SemSeg Pipeline (data.py): https://github.com/HasnainRaz/SemSegPipeline\n",
    "- Semantic Segmentation and the Dataset (prepare dataset): https://d2l.ai/chapter_computer-vision/semantic-segmentation-and-dataset.html\n",
    "- PLC segmentation (custom callback) : https://github.com/ika-rwth-aachen/PCLSegmentation/blob/main/pcl_segmentation/utils/callbacks.py\n",
    "- Human Image Segmentation (data and application ) : https://github.com/nikhilroxtomar/Human-Image-Segmentation-with-DeepLabV3Plus-in-TensorFlow\n",
    "- Deep Resnet and Resnet++ (pytorch) : https://github.com/rishikksh20/ResUnet\n",
    "- Deep Resnet and Resnet++ (original repo and tf) : https://github.com/DebeshJha/ResUNetPlusPlus\n",
    "- Loss functions for image segmentation : https://github.com/JunMa11/SegLoss\n",
    "- Albumentations with tf.data : https://colab.research.google.com/drive/1uUH-asz3CFxlvld8uGx-m3crr4Iepp3u#scrollTo=LWO057_PshWr\n",
    "- DeepLabv3 with (reference_repo) : https://github.com/lattice-ai/DeepLabV3-Plus\n",
    "- tf-segmentation-segmentation (reference_repo) : https://github.com/baudcode/tf-semantic-segmentation/tree/feature/line-detection/tf_semantic_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import Conv2D,Conv1D\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import initializers\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tf_seg.utils import TensorLike, Tensor, FloatTensorLike,AcceptableDTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resunet_pp.ResUnetPlusPlus().build_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._attention_block(np.ones((1, 64, 64, 3)), np.ones((1, 32, 32, 15)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(Layer):\n",
    "    def __init__(self,n_filter,activation='relu',kernel_size=3,name=\"residual\",**kwargs):\n",
    "        super().__init__(name=name)\n",
    "        assert n_filter is not None ,'n_filter must be specified'\n",
    "\n",
    "        self.activation_name = activation\n",
    "        self.n_filter = n_filter\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "       \n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.activation = keras.layers.Activation(self.activation_name)\n",
    "        \n",
    "        self.conv1 = Conv2D(filters=self.n_filter, kernel_size=self.kernel_size, padding=\"same\",kernel_initializer=initializers.Ones())\n",
    "        self.batch_norm1 = keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv2 = Conv2D(filters=self.n_filter, kernel_size=self.kernel_size,padding=\"same\",kernel_initializer=initializers.Ones())\n",
    "        self.batch_norm2 = keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv_skip = Conv2D(filters=self.n_filter, kernel_size=(1, 1), padding=\"same\",kernel_initializer=initializers.Ones())\n",
    "        self.batch_norm_skip = keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.add = keras.layers.Add()\n",
    "\n",
    "\n",
    "    def call(self,inputs):\n",
    "        \n",
    "        x = self.batch_norm1(inputs)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x_i = self.conv_skip(inputs)\n",
    "        x_i = self.batch_norm_skip(x_i)\n",
    "        x = self.add([x, x_i])\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_layer = ResidualLayer(n_filter=1, kernel_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = keras.Input((1, 64, 64, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = residual_layer(i)\n",
    "o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=i, outputs=o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_layer(np.ones((1, 2, 2, 3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dice Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_seg.losses import dice_coef, dice_loss, DiceLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([[1, 0, 1, 0, 0]], dtype=float)\n",
    "p = np.array([[1, 0, 1, 0, 1], [1, 0, 1, 0, 0]], dtype=float)\n",
    "i = 100000\n",
    "p = np.random.choice(2, size=(i)).astype(float)\n",
    "t = np.random.choice(2, size=(i)).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = DiceLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss(t, p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from tensorflow.keras.metrics import BinaryAccuracy,binary_accuracy\n",
    "from tensorflow.keras.metrics import MeanMetricWrapper\n",
    "from tensorflow.keras.metrics import MeanIoU,Precision\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_seg.metrics import DiceScore\n",
    "from tf_seg.metrics import MeanMetricWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_score = DiceScore(name=\"dice_score_spece\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([[1, 0, 1, 0, 0]], dtype=float)\n",
    "p = np.array([[1, 0, 1, 0, 1], [1, 0, 1, 0, 0]], dtype=float)\n",
    "i = 100000\n",
    "p = np.random.choice(2, size=(i)).astype(float)\n",
    "t = np.random.choice(2, size=(i)).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_score = MeanIoU(num_classes=2, name=\"mean_iou\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_score(t, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_score(t, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_score.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_score.get_weights()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "import tensorflow as tf\n",
    "#from tensorflow_addons.image import rotate\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "from tf_seg.data import DataLoader\n",
    "from tf_seg.data import get_camvid_data_loader\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_image_paths = glob(\"./dataset_v1.0/train_image/*.jpg\")\n",
    "# train_annot_paths = glob(\"./dataset_v1.0/train_annot/*.jpg\")\n",
    "\n",
    "# camvid\n",
    "train_image_paths = sorted(glob(\"./dataset/camvid/train/*.png\"))\n",
    "train_annot_paths = sorted(glob(\"./dataset/camvid/train_labels/*.png\"))\n",
    "\n",
    "assert len(train_image_paths) > 0 and len(train_annot_paths) > 0, f\"No training images found  train image length {len(train_image_paths)} train annot length {len(train_annot_paths)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annot_paths[:3], train_image_paths[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Road,128, 64, 128\n",
    "# RoadShoulder,128, 128, 192\n",
    "# Sidewalk,0, 0, 192\n",
    "# SignSymbol,192, 128, 128\n",
    "# Sky,128, 128, 128\n",
    "\n",
    "CLASSES = [\"Road\", \"Sky\"]\n",
    "palette = [(128, 64, 128), (128, 128, 128)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_loader = DataLoader(train_image_paths, train_annot_paths, image_size=(512, 512), output_type=(tf.float32, tf.float32),one_hot_encoding=True,channels = (3,3),palette=palette,background_adding=True)\n",
    "\n",
    "#dataset = data_loader.load_data(batch_size=12,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (i,m)  in dataset.take(3):\n",
    "#     #plt.imshow(i.numpy().astype(int))\n",
    "#     plt.subplot(1,2,1)\n",
    "#     plt.imshow(tf.cast(i,tf.uint8))\n",
    "#     plt.subplot(1,2,2)\n",
    "#     plt.imshow(m)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_SIZE = 512\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(IM_SIZE, IM_SIZE),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.HorizontalFlip(),\n",
    "                A.VerticalFlip(),\n",
    "            ],\n",
    "            p=0.3,\n",
    "        ),\n",
    "        A.RandomRotate90(),\n",
    "        # A.RandomGridShuffle(grid=(3, 3), always_apply=False, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, always_apply=False, p=0.5),\n",
    "        # A.Cutout(num_holes=8, max_h_size=8, max_w_size=8, fill_value=0, always_apply=False, p=0.5),\n",
    "        A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), always_apply=False, p=0.5),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.RandomBrightnessContrast??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_albument(image, mask):\n",
    "    data = {\"image\": image, \"mask\": mask}\n",
    "    data = transforms(**data)\n",
    "    image = data[\"image\"]\n",
    "    mask = data[\"mask\"]\n",
    "    # image = tf.cast(image/255., tf.float32)\n",
    "    return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def process_data(image, mask):\n",
    "    aug_img, aug_mask = tf.numpy_function(func=aug_albument, inp=[image, mask], Tout=[tf.uint8, tf.uint8])\n",
    "    return aug_img, aug_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "    train_image_paths, train_annot_paths, image_size=(512, 512), output_type=(tf.uint8, tf.float32), one_hot_encoding=True, channels=(3, 3), palette=palette, background_adding=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_loader.load_data(batch_size=20, shuffle=True, transform_func=process_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t():\n",
    "    for (i, m) in train_dataset.take(100):\n",
    "        pass\n",
    "        # print(i.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti = time.time()\n",
    "t()\n",
    "print(time.time() - ti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti = time.time()\n",
    "t()\n",
    "print(time.time() - ti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "train_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, m = train_dataset.take(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Camvdi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "\n",
    "source = \"http://web4.cs.ucl.ac.uk/staff/g.brostow/MotionSegRecData/files/701_StillsRaw_full.zip\"\n",
    "target = '701_StillsRaw_full.zip'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reporthook(count, block_size, total_size):\n",
    "    global start_time\n",
    "    if count == 0:\n",
    "        start_time = time.time()\n",
    "        return\n",
    "    duration = time.time() - start_time\n",
    "    progress_size = int(count * block_size)\n",
    "    speed = progress_size / (1024.**2 * duration)\n",
    "    percent = count * block_size * 100. / total_size\n",
    "    sys.stdout.write(\"\\r%d%% | %d MB | %.2f MB/s | %d sec elapsed\" %\n",
    "                    (percent, progress_size / (1024.**2), speed, duration))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "if not os.path.isdir('701_StillsRaw') and not os.path.isfile('701_StillsRaw_full.zip'):\n",
    "    \n",
    "    if (sys.version_info < (3, 0)):\n",
    "        import urllib\n",
    "        urllib.urlretrieve(source, target, reporthook)\n",
    "    \n",
    "    else:\n",
    "        import urllib.request\n",
    "        urllib.request.urlretrieve(source, target, reporthook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(source, target,lambda c,b,t: print(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from tensorflow.keras.utils import image_dataset_from_directory\n",
    "#from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "#from tensorflow_addons.image import rotate\n",
    "#from glob import glob\n",
    "#from tf_seg.data import DataLoader\n",
    "\n",
    "from tf_seg.data.aug import jax_random_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"dataset/camvid/train/0001TP_009210.png\"\n",
    "mask_path = \"dataset/camvid/train_labels/0001TP_009210_L.png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_content = tf.io.read_file(image_path)\n",
    "mask_content = tf.io.read_file(mask_path)\n",
    "\n",
    "image = tf.image.decode_png(image_content, channels=3)\n",
    "mask = tf.image.decode_png(mask_content, channels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, m = image.numpy(), mask.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "jax_random_crop(i,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union\n",
    "from warnings import warn\n",
    "\n",
    "import os \n",
    "from omegaconf import DictConfig, ListConfig, OmegaConf\n",
    "\n",
    "from tf_seg.config import get_config\n",
    "from tf_seg.models import get_model_builder\n",
    "\n",
    "from tf_seg.data import get_camvid_data_loader\n",
    "from tf_seg.data import get_data_loader    \n",
    "\n",
    "#from tf_seg.config import CONFIG_LOAD_STYLE_LIB,CONFIG_FILE_EXTENSION,CONFIG_STORE_PATH\n",
    "#from tf_seq.config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config function\n",
    "\n",
    "# parameter\n",
    "config_name = \"test\"\n",
    "config_path = None\n",
    "\n",
    "# constant parameters get from constant.py\n",
    "# CONFIG_STORE_PATH = \"./config\"\n",
    "# CONFIG_FILE_EXTENSION = \".yaml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(config_name, config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_model function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_builder = get_model_builder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_seg.models import model_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "from tensorflow.keras.models import Model\n",
    "from omegaconf import DictConfig, ListConfig\n",
    "\n",
    "\n",
    "def pascal_case_to_snake_case(s:str)->str:\n",
    "    \"\"\"Convert class name to snake case name.\"\"\"\n",
    "    return \"\".join([\"_\"+i.lower() if i.isupper() else i for i in s]).lstrip(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_builder(config:Union[DictConfig, ListConfig])->Model:\n",
    "    \"\"\"Get keras model from config file\"\"\"\n",
    "    class_name = pascal_case_to_snake_case(config.model.class_name)\n",
    "    model = model_lib[class_name]\n",
    "\n",
    "    model_config = config.model.copy()    \n",
    "    model_config.pop(\"class_name\")\n",
    "    \n",
    "    return model(**model_config) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders =get_data_loader(config,train_data=True,val_data=True,test_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = loaders[0].load_data(batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset.take(1):\n",
    "    plt.imshow(tf.squeeze(i[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_aug function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_seg.transformers import get_transformer\n",
    "from tf_seg.utils import AlbumentatiosWrapper\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(config_name, config_path)\n",
    "aug_config = config.augmentation\n",
    "aug_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = get_transformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_config = config.augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = get_transformer(config)\n",
    "lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_config function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(\n",
    "    config_filename: Optional[str] = None, config_path: Optional[Union[Path, str]] = None, config_file_extension: Optional[str] = CONFIG_FILE_EXTENSION, config_store_path: \"str\" = CONFIG_STORE_PATH\n",
    ") -> Union[DictConfig, ListConfig]:\n",
    "\n",
    "    \"\"\"\n",
    "    Get configurable parameters from config file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config_filename : str, optional\n",
    "        Name of the config file. The default is None.\n",
    "    config_path : Union[Path, str], optional\n",
    "        Path of the config file. The default is None.\n",
    "    config_file_extension : str, optional\n",
    "        File extension of the config file. The default is \".yaml\".\n",
    "    config_store_path : str\n",
    "        Path of the config store. The default is \"./config\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    config : Union[DictConfig, ListConfig]\n",
    "        Configurable parameters.\n",
    "\n",
    "    \"\"\"\n",
    "    assert os.path.isdir(config_store_path), f\"{config_store_path} is not a directory\"\n",
    "\n",
    "    if config_path is None:\n",
    "        config_path = Path(f\"{config_store_path}/{config_name}{config_file_extension}\")\n",
    "\n",
    "    config = OmegaConf.load(config_path)\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config(config_name)\n",
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_base_config_exist(config: Union[DictConfig, ListConfig]) -> bool:\n",
    "    \"\"\"check if base config exist in config file\"\"\"\n",
    "\n",
    "    if \"base\" in config.keys():\n",
    "        if config[\"base\"] is None:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_yaml_style_config(yaml_path: str) -> Union[DictConfig, ListConfig]:\n",
    "    \"\"\"load config from yaml file\"\"\"\n",
    "    assert os.path.isfile(yaml_path), f\"{yaml_path} is not a file\"\n",
    "    config = OmegaConf.load(yaml_path)\n",
    "    return config\n",
    "\n",
    "\n",
    "def load_module_style_config(module_path: str) -> Union[DictConfig, ListConfig]:\n",
    "    \"\"\"load config from python module\"\"\"\n",
    "    raise NotImplementedError(\"load_mdule_style_config is not implemented\")\n",
    "\n",
    "\n",
    "def load_base_config(config):\n",
    "    \"\"\"load base config from base parameter\"\"\"\n",
    "\n",
    "    # find file extension\n",
    "    load_style = os.path.splitext(config[\"base\"])[1]\n",
    "\n",
    "    print(\"load style :\", load_style)\n",
    "\n",
    "    if load_style == \".py\":  # module style\n",
    "        raise NotImplementedError(\"load_style is not implemented\")\n",
    "\n",
    "    elif load_style == \".yaml\" or load_style == \".yml\":\n",
    "        return load_yaml_style_config(config[\"base\"])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"load_style is not valid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extact_config(config: Union[DictConfig, ListConfig]) -> Union[DictConfig, ListConfig]:\n",
    "    \"\"\"\"Extract parent base config and merge with child configs\"\"\"\n",
    "\n",
    "    first_config_keys = list(config.keys())\n",
    "    buffer_config_dict = {}\n",
    "    f_config = config.copy()\n",
    "\n",
    "    for first_k in first_config_keys:\n",
    "\n",
    "        sub_config = f_config[first_k]\n",
    "\n",
    "        if check_base_config_exist(sub_config):\n",
    "            buffer_config = load_base_config(sub_config)\n",
    "            sub_config.pop(\"base\")\n",
    "            buffer_config.merge_with(sub_config)\n",
    "            sub_config = buffer_config\n",
    "\n",
    "        else:\n",
    "            if \"base\" in sub_config.keys():\n",
    "                sub_config.pop(\"base\")\n",
    "\n",
    "        f_config[first_k] = sub_config\n",
    "\n",
    "    return f_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_config[\"augmentation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_config.pop(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b_k in b.keys():\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.merge_with(sub_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# from tensorflow.keras import initializers\n",
    "# from tensorflow.keras.layers import Conv2D,Conv1D\n",
    "# from tensorflow.keras.layers import BatchNormalization\n",
    "# from tensorflow.keras.layers import Layer\n",
    "# from tensorflow.keras.losses import Loss\n",
    "# from tensorflow.keras import Model\n",
    "# from tensorflow.keras import initializers\n",
    "\n",
    "from tf_seg.models import Unet,ResUnet\n",
    "from tf_seg.backbones import get_backbone \n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.python.keras.engine import keras_tensor\n",
    "from tensorflow.keras.layers import Conv2D,Conv2DTranspose,Concatenate\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Done Work**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = None #\"EfficientNetB1\"#\"ResNet50\"\n",
    "\n",
    "n_filters = [8, 16, 32, 64, 128,256]#, 512, 1024]\n",
    "model=Unet(backbone=backbone,n_filters=n_filters,input_shape=(256, 256,3)).build_model()\n",
    "\n",
    "out = model(np.ones((1,256,256,3)))\n",
    "keras.utils.plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append(\"appendix/keras-unet-collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_unet_collection import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.unet_2d((1024, 1024, 3), [16, 32,312], n_labels=2,backbone=\"EfficientNetB1\")\n",
    "#model = models.unet_2d((1024, 1024, 3), [64, 128, 256, 512, 1024], n_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input((1024,1024,3))\n",
    "#backbone = get_backbone(\"ResNet50\",\"imagenet\",input_tensor,depth=4,freeze_backbone=True,freeze_batch_norm=False)\n",
    "backbone = get_backbone(\"ResNet50V2\",\"imagenet\",input_tensor,depth=5,freeze_backbone=True,freeze_batch_norm=False)\n",
    "backbone(input_tensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model= Unet(backbone=\"ResNet50\",n_filters=[16,32,64]).build_model()\n",
    "model= Unet(backbone=None,n_filters=[32,64,128,256,512,1024]).build_model()\n",
    "\n",
    "\n",
    "connection_list = model[0]\n",
    "decode_n_filters= model[1]\n",
    "inputs = model[2]\n",
    "bridge = model[3]\n",
    "\n",
    "final_activation = \"relu\"\n",
    "n_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = \"ResNet50\"\n",
    "n_filters = [32,64,128,256,512]\n",
    "#encoder_output= Unet(backbone=None,n_filters=[32,64,128,256,512]).build_model()\n",
    "encoder_output= Unet(backbone=backbone,n_filters=n_filters).build_model()\n",
    "\n",
    "inputs = encoder_output[0]\n",
    "bridge = encoder_output[-1]\n",
    "connection_list = encoder_output[1:-1][::-1]\n",
    "inputs,bridge,connection_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if backbone is None:\n",
    "#     decoder_n_filters = n_filters[:-1][::-1]\n",
    "decoder_n_filters = n_filters[:-1][::-1]\n",
    "decoder_n_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = bridge\n",
    "\n",
    "for n,c in enumerate(connection_list):\n",
    "\n",
    "    print(\"connection :\",c.shape,n)\n",
    "    d=Conv2DTranspose(decoder_n_filters[n], (3, 3), padding=\"same\", strides=(2, 2))(d)\n",
    "    print(d.shape)\n",
    "    d = Concatenate()([d,c])\n",
    "    print(\"after concat :\",d.shape)\n",
    "    d = Unet()._conv_block(d,decoder_n_filters[n],pool=False)\n",
    "\n",
    "print(\"last shape :\",d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = len(connection_list)\n",
    "\n",
    "if len(decoder_n_filters)>depth:\n",
    "    n_remain_block = len(decoder_n_filters) - depth\n",
    "    print(decoder_n_filters[-1*n_remain_block:])\n",
    "\n",
    "    remain_decoder_n_filter=decoder_n_filters[-1*n_remain_block:]\n",
    "    for fltr in remain_decoder_n_filter:\n",
    "        print(fltr)\n",
    "        d=Conv2DTranspose(fltr, (3, 3), padding=\"same\", strides=(2, 2))(d)\n",
    "        d = Unet()._conv_block(d,fltr,pool=False)\n",
    "    \n",
    "print(\"last shape :\", d.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not backbone is None:\n",
    "# not bigger n_filters 6\n",
    "#  not smaller n_filters 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, List\n",
    "from omegaconf import DictConfig, ListConfig\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "    \"Trainer class for training a model\"\n",
    "\n",
    "    def __init__(self, config: Union[DictConfig, ListConfig], model: Model,\n",
    "                 train_data: Dataset, val_data: Optional[Dataset] = None,\n",
    "                 callbacks: Optional[List[Callback]] = None) -> None:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config: Union[DictConfig, ListConfig]\n",
    "            Configuration for training\n",
    "        model: tf.keras.Model\n",
    "            Model to be trained\n",
    "        train_data: tf.data.Dataset\n",
    "            Dataset for training\n",
    "        val_data: tf.data.Dataset,default: None\n",
    "            Dataset for validation\n",
    "        callbacks: tf.keras.callbacks.Callback,default: None\n",
    "            keras Callbacks for training\n",
    "\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.model:Model = model\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.callbacks = callbacks\n",
    "\n",
    "        self._check_params()\n",
    "\n",
    "    def _check_params(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        \"Train tf keras model\"\n",
    "        \n",
    "        self.model.fit(self.train_data,\n",
    "                       epochs=self.config.epochs,\n",
    "                       callbacks=self.callbacks,\n",
    "                       validation_data=self.val_data)\n",
    "\n",
    "    def evaluate(self):\n",
    "        if self.val_data: \n",
    "            self.model.evaluate(self.val_data)\n",
    "        else:\n",
    "            raise ValueError(\"Validation data is not provided\")\n",
    "            \n",
    "    def save(self, filename):\n",
    "        self.model.save(filename)\n",
    "\n",
    "    def test(self, data):\n",
    "        self.model.evaluate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Starting training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau,Callback\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay,PiecewiseConstantDecay\n",
    "from tf_seg.utils import  snake_case_to_pascal_case\n",
    "from tf_seg.callbacks import get_callbacks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "config_name = \"test\"\n",
    "config_path = None\n",
    "config = get_config(config_name, config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_callbacks(config.callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\"a\":12}\n",
    "b = {\"b\":12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.less(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_config  = config.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in c_config.keys():\n",
    "    print(snake_case_to_pascal_case(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100\n",
    "x1 = np.random.randint(1,10,(size,10,1))\n",
    "y1 = x1.max(axis=1) #+ np.random.rand(size)\n",
    "x1 = np.concatenate((x1,np.ones((size,1,1))),axis=1)\n",
    "\n",
    "\n",
    "x2 = np.random.randint(1,10,(size,10,1))\n",
    "y2 = x2.min(axis=1) #+ np.random.rand(size)\n",
    "x2 = np.concatenate((x2,np.zeros((size,1,1))),axis=1)\n",
    "\n",
    "x = np.concatenate((x1,x2))\n",
    "y = np.concatenate((y1,y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_dim):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(1, input_dim=input_dim,use_bias=False))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Nadam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.mae,\n",
    "        metrics=[\"mae\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model(input_dim=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomCallbacks(keras.callbacks.Callback):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.epoch_level = 10\n",
    "#     def on_train_begin(self,logs=None):\n",
    "#         print(self.model)\n",
    "#         #print(logs.keys())\n",
    "            \n",
    "#     def on_epoch_end(self,epoch,logs=None):\n",
    "#         #print(\"training epoch end:\",logs.keys())\n",
    "#         print(self.dataset)\n",
    "#         if epoch% self.epoch_level==0:\n",
    "#             print(f\"Epoch : {epoch} , loss : {logs}\")\n",
    "#     # def on_batch_end(self,batch,logs=None):\n",
    "#     #     print(\"     => training batch end:\",logs.keys())\n",
    "class MeasureTotalTime(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(self.model)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.total_time = time.time() - self.start_time\n",
    "        print(\"Total training time: %s\" % self.total_time)\n",
    "\n",
    "\n",
    "class UpdateBestWeights(Callback):\n",
    "    def __init__(self, metric_name: str, mode:str):\n",
    "        \"\"\"\n",
    "        Update best weights on end of training\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        metric_name : str\n",
    "            Name of the metric to use for determining the best weights\n",
    "        mode : str\n",
    "            One of {min, max}\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "       \n",
    "        self.metric_name = metric_name\n",
    "        self.best_weights = None\n",
    "        self.best_metric = None\n",
    "\n",
    "        if mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best_metric = -np.Inf\n",
    "        elif mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best_metric = np.Inf\n",
    "        else:\n",
    "            raise ValueError('Mode {} not understood'.format(mode))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        metric = logs[self.metric_name]\n",
    "        if self.monitor_op(metric, self.best_metric):\n",
    "            self.best_metric = metric\n",
    "            self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_train_end(self, logs=None):       \n",
    "        self.model.set_weights(self.best_weights)\n",
    "     \n",
    "\n",
    "\n",
    "update_model = UpdateBestWeights(metric_name='val_loss',mode='min')\n",
    "tl_tm = MeasureTotalTime() \n",
    "lr = LearningRateScheduler(PolynomialDecay(0.9,100,power=1),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x, y,validation_data=(x,y), epochs=10,batch_size=120,callbacks=[tl_tm,update_model],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.losses.mae(y.reshape(-1), model.predict(x).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x,y,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('keras-tracker')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fcdb1ca4abafb692adb0201f899273c9a29e97de891f135b2a2e0898b8b3fe5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
